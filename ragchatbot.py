# -*- coding: utf-8 -*-
"""RAGChatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hmDoSAh8S8-RnqzgxqGwBmhNbO6ubciI
"""

!pip install langchain transformers faiss-cpu sentence-transformers pypdf streamlit -q
!pip install accelerate bitsandbytes -q
!pip install langchain-community



from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

pdf_path = "researchpaper.pdf"  # Already uploaded
loader = PyPDFLoader(pdf_path)
documents = loader.load()

splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = splitter.split_documents(documents)


from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(docs, embedding_model)

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.llms import HuggingFacePipeline

model_name = "HuggingFaceH4/zephyr-7b-beta"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", torch_dtype="auto")

pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=300, do_sample=True)
llm = HuggingFacePipeline(pipeline=pipe)



from langchain.chains import RetrievalQA

retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 4})
qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)


print("âœ… Ready! Ask questions based on 'researchpaper.pdf'. Type 'exit' to quit.")
while True:
    query = input("\nEnter your question: ")
    if query.lower() == 'exit':
        print("ðŸ‘‹ Exiting...")
        break
    answer = qa_chain.run(query)
    print(f"\nðŸ”Ž Answer:\n{answer}")